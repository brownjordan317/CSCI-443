{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "This notebook will serve as the primary means of documentation for creating an object detection model\n",
    "_______________________________________________________________________________________________________________________________________________\n",
    "\n",
    "### Full group includes:\n",
    "- Jordan Brown\n",
    "- Dylan Roy\n",
    "- Maxwell Barret\n",
    "- Julia Dewhurt\n",
    "\n",
    "### Primary notebook contributers:\n",
    "- \"\"\n",
    "- \"\"\n",
    "_______________________________________________________________________________________________________________________________________________\n",
    "\n",
    "***The primary notebook contributers are the group members who were assigned to this specific task. All group members will work in collaboration to create a final working product. However, the nature of this project calls for the full group to be assigned to primary tasks.***\n",
    "\n",
    "## Notebook Goal/Purpose\n",
    "\n",
    "This notebook will be used to create a model for traffic based object detection. This will involve using labeled image and video feeds to determine what traffic objects look like. This model will be able to detect and track traffic counts at individual intersections. This tracking will be used to create a dataset which can hopefully be used to train a traffic prediction for the traffic analysis portion of this project.\n",
    "_______________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO dataset already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a specified destination.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(destination, 'wb') as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {destination}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"\n",
    "    Extract a zip file to a specified directory.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Extracted {zip_file} to {extract_to}\")\n",
    "\n",
    "def download_and_extract_coco(destination_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the COCO dataset (2017 version) images and annotations.\n",
    "    \"\"\"\n",
    "    # Create destination directory if it doesn't exist\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    # Define COCO dataset URLs\n",
    "    coco_urls = {\n",
    "        'train2017': 'http://images.cocodataset.org/zips/train2017.zip',\n",
    "        'val2017': 'http://images.cocodataset.org/zips/val2017.zip',\n",
    "        'annotations': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "    }\n",
    "\n",
    "    # Download images and annotations\n",
    "    for name, url in coco_urls.items():\n",
    "        file_path = os.path.join(destination_dir, f\"{name}.zip\")\n",
    "        \n",
    "        print(f\"Downloading {name}...\")\n",
    "        download_file(url, file_path)\n",
    "        \n",
    "        # Extract the downloaded zip files\n",
    "        print(f\"Extracting {name}...\")\n",
    "        extract_zip(file_path, destination_dir)\n",
    "\n",
    "        # Remove zip files after extraction\n",
    "        os.remove(file_path)\n",
    "\n",
    "    print(\"COCO dataset downloaded and extracted successfully!\")\n",
    "\n",
    "# Check if the COCO dataset has already been downloaded\n",
    "if os.path.exists('COCO'):\n",
    "    print(\"COCO dataset already exists. Skipping download and extraction.\")\n",
    "    exit()\n",
    "else:\n",
    "    \n",
    "    # Specify the directory to save the dataset\n",
    "    destination_dir = 'COCO'\n",
    "\n",
    "    # Start downloading and extracting COCO dataset\n",
    "    download_and_extract_coco(destination_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=15.93s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brownjordan317/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/brownjordan317/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root, ann_file, transforms=None, selected_classes=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.transforms = transforms\n",
    "        self.selected_classes = selected_classes\n",
    "        if self.selected_classes:\n",
    "            self.selected_classes = [\n",
    "                self.coco.getCatIds(catNms=[cls])[0] for cls in self.selected_classes\n",
    "            ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "\n",
    "        # Extract bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            if self.selected_classes and ann[\"category_id\"] not in self.selected_classes:\n",
    "                continue\n",
    "            x, y, w, h = ann[\"bbox\"]  # COCO format: [x, y, width, height]\n",
    "            if w > 0 and h > 0:  # Ensure valid boxes\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                labels.append(self.selected_classes.index(ann[\"category_id\"]) + 1)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Handle empty annotations\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        # Image ID\n",
    "        image_id = torch.tensor([img_id])\n",
    "\n",
    "        # Area and iscrowd\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd,\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Update the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=1, lr=0.005):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, targets in train_loader:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(imgs, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Validation (optional)\n",
    "        if val_loader is not None:\n",
    "            validate_model(model, val_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            _ = model(imgs)  # No loss calculation, only forward pass\n",
    "\n",
    "\n",
    "# Paths to COCO dataset\n",
    "root = \"COCO/train2017\"\n",
    "ann_file = \"COCO/annotations/instances_train2017.json\"\n",
    "\n",
    "# Define the classes to detect\n",
    "selected_classes = [\"car\", \"truck\", \"motorcycle\", \"bus\"]\n",
    "\n",
    "# Define dataset and dataloaders\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "dataset = COCODataset(root, ann_file, transforms=transforms, selected_classes=selected_classes)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_root = \"COCO/val2017\"\n",
    "val_ann_file = \"COCO/annotations/instances_val2017.json\"\n",
    "val_dataset = COCODataset(val_root, val_ann_file, transforms=transforms, selected_classes=selected_classes)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Get model\n",
    "num_classes = len(selected_classes) + 1  # Add 1 for the background class\n",
    "model = get_model(num_classes)\n",
    "\n",
    "# Train\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "train_model(model, train_loader, val_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
