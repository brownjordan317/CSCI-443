{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Implementation\n",
    "\n",
    "This code will contain the implementation for our pipeline that combines both our object detection and our traffic prediction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.callable_implementation import write_csv\n",
    "from object_detection.callable_implementation import process_image\n",
    "from object_detection.callable_implementation import get_dt_info\n",
    "from object_detection.callable_implementation import load_model\n",
    "from object_detection.callable_implementation import load_tensor_img\n",
    "from object_detection.render_video import images_to_video_ffmpeg\n",
    "\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "from queue import Queue\n",
    "import shutil\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model, vehicle_classes, device = load_model()\n",
    "loaded_model = joblib.load(\"/home/brownjordan317/fall_2024/CSCI443/Github/CSCI-443/traffic_prediction_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.26.40.147:8080\n"
     ]
    }
   ],
   "source": [
    "# read ip from text file\n",
    "with open('my_ip.txt', 'r') as f:\n",
    "    ip = f.read().strip()\n",
    "print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counts(data, image, pred, output_folder, image_file):\n",
    "    # Resize the image\n",
    "    image = imutils.resize(image, width=800)\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Add white padding to the right side for annotations\n",
    "    padding_width = 300  # Adjust as needed for the text\n",
    "    padded_image = np.full((height, width + padding_width, 3), 255, dtype=np.uint8)  # White background\n",
    "    padded_image[:, :width] = image  # Place the original image on the left\n",
    "\n",
    "    # Create annotations\n",
    "    annotations = [\n",
    "        f\"Time: {data['Time']}\",\n",
    "        f\"Day: {data['Day of the week']}\",\n",
    "        f\"Car Count: {data['CarCount']}\",\n",
    "        f\"Bike Count: {data['BikeCount']}\",\n",
    "        f\"Bus Count: {data['BusCount']}\",\n",
    "        f\"Truck Count: {data['TruckCount']}\",\n",
    "        f\"Total: {data['Total']}\",\n",
    "        f\"Prediction: {pred}\"\n",
    "    ]\n",
    "\n",
    "    # Set annotation settings\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.6\n",
    "    font_color = (0, 0, 0)  # Black text for contrast with white background\n",
    "    line_type = cv2.LINE_AA\n",
    "\n",
    "    # Determine the starting position for annotations (right side of the padded image)\n",
    "    x_offset = width + 10  # Leave a small margin from the start of the padding\n",
    "    y_offset = 30  # Initial vertical offset\n",
    "    line_height = 25  # Space between each annotation\n",
    "\n",
    "    # Add each annotation to the image\n",
    "    for i, text in enumerate(annotations):\n",
    "        y_position = y_offset + i * line_height\n",
    "        cv2.putText(padded_image, text, (x_offset, y_position), font, font_scale, font_color, 1, line_type)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 111\u001b[0m\n\u001b[1;32m    101\u001b[0m         image_queue\u001b[38;5;241m.\u001b[39mput(filename)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# # Display the image on the Android camera window\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# cv2.imshow(\"Android_cam\", img)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Sleep to avoid high CPU usage (sleeping 1ms in case timing isn't perfect)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Stop the annotation and display threads gracefully\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RUNNING = True\n",
    "\n",
    "# Replace the below URL with your own. Make sure to add \"/shot.jpg\" at the last.\n",
    "url = f\"http://{ip}/shot.jpg\"\n",
    "\n",
    "# Initialize variables\n",
    "fps = 4  # Desired FPS for the output video\n",
    "out_dir = \"test_images\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Queue for storing images that need to be annotated and displayed\n",
    "image_queue = Queue()\n",
    "annotated_image_queue = Queue()\n",
    "\n",
    "# Function to process the image\n",
    "def annotate(image_path):\n",
    "    # print(f\"Annotating image: {image_path}\")\n",
    "    output_path = 'live_stream'\n",
    "    image_base_name = os.path.basename(image_path).split('.')[0]\n",
    "    outfile_name = f'{image_base_name}_annotated.jpg'\n",
    "    img, img_tensor = load_tensor_img(image_path, device)\n",
    "    data, out_image = process_image(model, vehicle_classes, img, img_tensor, output_path, outfile_name)\n",
    "    time, day = get_dt_info()\n",
    "    data[\"Time\"] = time\n",
    "    data[\"Day of the week\"] = day\n",
    "    # send to prediction model\n",
    "    # print low, normal, high\n",
    "    write_csv(data, \"test.csv\")\n",
    "    \n",
    "    headers = ['Time', 'Day of the week', 'CarCount', 'BikeCount', 'BusCount', 'TruckCount', 'Total']\n",
    "    X_pred = [data.get(header, \"\") for header in headers]\n",
    "    # convert to df\n",
    "    X_pred = pd.DataFrame([X_pred], columns=headers)\n",
    "    le = LabelEncoder()\n",
    "    X_pred['Day of the week'] = le.fit_transform(X_pred['Day of the week'])\n",
    "\n",
    "    pred = int(loaded_model.predict(X_pred)[0])\n",
    "    \n",
    "    if pred == 0:\n",
    "        pred = \"heavy\"\n",
    "    elif pred == 1:\n",
    "        pred = \"high\"\n",
    "    elif pred == 2:\n",
    "        pred = \"low\"\n",
    "    elif pred == 3:\n",
    "        pred = \"normal\"\n",
    "    \n",
    "    # Save the annotated image\n",
    "    annotated_image_path = os.path.join(out_dir, outfile_name)\n",
    "    out_image = add_counts(data, out_image, pred)\n",
    "    cv2.imwrite(annotated_image_path, out_image)\n",
    "\n",
    "    # Put the annotated image path in the annotated queue\n",
    "    annotated_image_queue.put(annotated_image_path)\n",
    "\n",
    "# Function to process images from the queue for annotation\n",
    "def process_annotations():\n",
    "    while True:\n",
    "        # Get an image path from the queue\n",
    "        img_path = image_queue.get()\n",
    "        if img_path is None:  # Sentinel value to stop the thread\n",
    "            break\n",
    "        # Call the annotation function for each image\n",
    "        annotate(img_path)\n",
    "        image_queue.task_done()\n",
    "\n",
    "# Function to display annotated images in a separate thread\n",
    "def display_annotated_images():\n",
    "    global RUNNING\n",
    "    while RUNNING:\n",
    "        # Get the annotated image path from the queue\n",
    "        annotated_img_path = annotated_image_queue.get()\n",
    "        if annotated_img_path is None:  # Sentinel value to stop the thread\n",
    "            break\n",
    "        \n",
    "        # Read and display the annotated image\n",
    "        annotated_img = cv2.imread(annotated_img_path)\n",
    "        cv2.imshow(\"Annotated Image\", annotated_img)\n",
    "        \n",
    "        # Press Esc key to exit the display window\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            RUNNING = False\n",
    "            break\n",
    "        annotated_image_queue.task_done()\n",
    "\n",
    "# Start the annotation processing thread\n",
    "annotation_thread = threading.Thread(target=process_annotations, daemon=True)\n",
    "annotation_thread.start()\n",
    "\n",
    "# Start the display thread\n",
    "display_thread = threading.Thread(target=display_annotated_images, daemon=True)\n",
    "display_thread.start()\n",
    "\n",
    "# Timing control to achieve 30 FPS\n",
    "prev_time = time.time()\n",
    "\n",
    "# While loop to continuously fetch data from the URL\n",
    "count = 0\n",
    "while RUNNING:\n",
    "    # Calculate the time difference\n",
    "    current_time = time.time()\n",
    "\n",
    "    img_resp = requests.get(url)\n",
    "    img_arr = np.array(bytearray(img_resp.content), dtype=np.uint8)\n",
    "    img = cv2.imdecode(img_arr, -1)\n",
    "    img = imutils.resize(img, width=1000, height=1800)\n",
    "\n",
    "    # Save the image with a timestamp to ensure uniqueness\n",
    "    filename = f\"{out_dir}/image_{count}.jpg\"\n",
    "    cv2.imwrite(filename, img)\n",
    "\n",
    "    # Add the image path to the queue for annotation\n",
    "    # if time diff is greater than 1/fps, add to queue\n",
    "    if (current_time - prev_time) >= 1 / fps:\n",
    "        image_queue.put(filename)\n",
    "\n",
    "    # # Display the image on the Android camera window\n",
    "    # cv2.imshow(\"Android_cam\", img)\n",
    "\n",
    "    # # Press Esc key to exit\n",
    "    # if cv2.waitKey(1) == 27:\n",
    "    #     break\n",
    "\n",
    "    # Sleep to avoid high CPU usage (sleeping 1ms in case timing isn't perfect)\n",
    "    time.sleep(1 / fps)\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "# Stop the annotation and display threads gracefully\n",
    "image_queue.put(None)  # Sentinel to stop the annotation thread\n",
    "annotation_thread.join()\n",
    "\n",
    "annotated_image_queue.put(None)  # Sentinel to stop the display thread\n",
    "display_thread.join()\n",
    "\n",
    "cv2.destroyWindow(\"Annotated Image\")\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "total_run_time = time.time() - prev_time\n",
    "num_seconds = int(total_run_time)\n",
    "# get num images in live_stream folder\n",
    "num_images = len([name for name in os.listdir(\"live_stream\") if os.path.isfile(os.path.join(\"live_stream\", name))])\n",
    "estimated_fps = num_images / num_seconds\n",
    "print(f\"Estimated FPS: {estimated_fps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/02 10:02:15.562759 cmd_run.go:1285: WARNING: cannot start document portal: dial unix /run/user/1000/bus: connect: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "date_time_info = get_dt_info()\n",
    "images_to_video_ffmpeg(\"live_stream\", f\"live_stream_{date_time_info}.mp4\", estimated_fps)\n",
    "\n",
    "# remove live_stream folder and its contents\n",
    "shutil.rmtree(\"live_stream\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
